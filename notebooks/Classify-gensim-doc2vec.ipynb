{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../src/data/raw_tweets.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  data['isTrump'] = [True if x == 'Twitter for Android' else False for x in data['source']]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>id_str</th>\n",
       "      <th>in_reply_to_user_id_str</th>\n",
       "      <th>is_retweet</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>isTrump</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>2327</td>\n",
       "      <td>2327</td>\n",
       "      <td>2327</td>\n",
       "      <td>2</td>\n",
       "      <td>2327</td>\n",
       "      <td>2327</td>\n",
       "      <td>2327</td>\n",
       "      <td>2327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>1835</td>\n",
       "      <td>1835</td>\n",
       "      <td>1835</td>\n",
       "      <td>4</td>\n",
       "      <td>1835</td>\n",
       "      <td>1835</td>\n",
       "      <td>1835</td>\n",
       "      <td>1835</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         created_at  favorite_count  id_str  in_reply_to_user_id_str  \\\n",
       "isTrump                                                                \n",
       "False          2327            2327    2327                        2   \n",
       "True           1835            1835    1835                        4   \n",
       "\n",
       "         is_retweet  retweet_count  source  text  \n",
       "isTrump                                           \n",
       "False          2327           2327    2327  2327  \n",
       "True           1835           1835    1835  1835  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load up the tweets\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "from data import raw_tweets\n",
    "\n",
    "all_data = raw_tweets.load()\n",
    "all_data = raw_tweets.label(all_data)\n",
    "\n",
    "all_data.groupby('isTrump').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_only = all_data[all_data.is_retweet ==False]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the tweets a bit. Can remove stops and/or lemmatize as needed.\n",
    "\n",
    "import pandas as pd\n",
    "from features import build_features\n",
    "    \n",
    "cleaned = pd.Series([build_features.clean_text(text, remove_stopwords = False, lemmatize = True)\n",
    "           for text in tweets_only['text']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "import multiprocessing\n",
    "from sklearn import utils\n",
    "\n",
    "def labelize_tweets_ug(tweets,label):\n",
    "    result = []\n",
    "    prefix = label\n",
    "    for i, t in zip(tweets.index, tweets):\n",
    "        result.append(LabeledSentence(t.split(), [prefix + '_%s' % i]))\n",
    "    return result\n",
    "  \n",
    "all_x_w2v = labelize_tweets_ug(cleaned, 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectors(model, corpus, size):\n",
    "    vecs = np.zeros((len(corpus), size))\n",
    "    n = 0\n",
    "    for i in corpus.index:\n",
    "        prefix = 'all_' + str(i)\n",
    "        vecs[n] = model.docvecs[prefix]\n",
    "        n += 1\n",
    "    return vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3974/3974 [00:00<00:00, 3788219.11it/s]\n",
      "100%|██████████| 3974/3974 [00:00<00:00, 4339537.65it/s]\n",
      "100%|██████████| 3974/3974 [00:00<00:00, 3947930.86it/s]\n",
      "100%|██████████| 3974/3974 [00:00<00:00, 4652013.42it/s]\n",
      "100%|██████████| 3974/3974 [00:00<00:00, 4613386.13it/s]\n",
      "100%|██████████| 3974/3974 [00:00<00:00, 4569123.93it/s]\n",
      "100%|██████████| 3974/3974 [00:00<00:00, 4608284.24it/s]\n",
      "100%|██████████| 3974/3974 [00:00<00:00, 4276081.09it/s]\n",
      "100%|██████████| 3974/3974 [00:00<00:00, 4513448.17it/s]\n",
      "100%|██████████| 3974/3974 [00:00<00:00, 4205946.02it/s]\n",
      "100%|██████████| 3974/3974 [00:00<00:00, 4593046.04it/s]\n",
      "100%|██████████| 3974/3974 [00:00<00:00, 4258600.94it/s]\n",
      "100%|██████████| 3974/3974 [00:00<00:00, 3889886.60it/s]\n",
      "100%|██████████| 3974/3974 [00:00<00:00, 4436562.18it/s]\n",
      "100%|██████████| 3974/3974 [00:00<00:00, 4393295.76it/s]\n",
      "100%|██████████| 3974/3974 [00:00<00:00, 4241263.13it/s]\n",
      "100%|██████████| 3974/3974 [00:00<00:00, 4792456.61it/s]\n",
      "100%|██████████| 3974/3974 [00:00<00:00, 4691293.02it/s]\n",
      "100%|██████████| 3974/3974 [00:00<00:00, 4431843.68it/s]\n",
      "100%|██████████| 3974/3974 [00:00<00:00, 4045670.90it/s]\n",
      "100%|██████████| 3974/3974 [00:00<00:00, 4791079.07it/s]\n",
      "100%|██████████| 3974/3974 [00:00<00:00, 4000999.54it/s]\n",
      "100%|██████████| 3974/3974 [00:00<00:00, 4529392.42it/s]\n",
      "100%|██████████| 3974/3974 [00:00<00:00, 4659816.63it/s]\n",
      "100%|██████████| 3974/3974 [00:00<00:00, 4506127.09it/s]\n",
      "100%|██████████| 3974/3974 [00:00<00:00, 4703206.57it/s]\n",
      "100%|██████████| 3974/3974 [00:00<00:00, 4073353.88it/s]\n",
      "100%|██████████| 3974/3974 [00:00<00:00, 3998120.44it/s]\n",
      "100%|██████████| 3974/3974 [00:00<00:00, 4633907.17it/s]\n",
      "100%|██████████| 3974/3974 [00:00<00:00, 4716515.02it/s]\n",
      "100%|██████████| 3974/3974 [00:00<00:00, 4803505.50it/s]\n",
      "100%|██████████| 3974/3974 [00:00<00:00, 4555387.84it/s]\n",
      "100%|██████████| 3974/3974 [00:00<00:00, 4765055.49it/s]\n",
      "100%|██████████| 3974/3974 [00:00<00:00, 4622341.68it/s]\n",
      "100%|██████████| 3974/3974 [00:00<00:00, 4604465.22it/s]\n",
      "100%|██████████| 3974/3974 [00:00<00:00, 4540496.89it/s]\n",
      "100%|██████████| 3974/3974 [00:00<00:00, 4683384.12it/s]\n",
      "100%|██████████| 3974/3974 [00:00<00:00, 4535554.86it/s]\n",
      "100%|██████████| 3974/3974 [00:00<00:00, 4234797.79it/s]\n",
      "100%|██████████| 3974/3974 [00:00<00:00, 4262957.57it/s]\n",
      "100%|██████████| 3974/3974 [00:00<00:00, 4614663.37it/s]\n",
      "100%|██████████| 3974/3974 [00:00<00:00, 4239105.82it/s]\n",
      "100%|██████████| 3974/3974 [00:00<00:00, 4250998.24it/s]\n",
      "100%|██████████| 3974/3974 [00:00<00:00, 4413069.66it/s]\n",
      "100%|██████████| 3974/3974 [00:00<00:00, 4632619.26it/s]\n",
      "100%|██████████| 3974/3974 [00:00<00:00, 4642942.65it/s]\n",
      "100%|██████████| 3974/3974 [00:00<00:00, 4224065.91it/s]\n",
      "100%|██████████| 3974/3974 [00:00<00:00, 4418919.43it/s]\n",
      "100%|██████████| 3974/3974 [00:00<00:00, 4265139.23it/s]\n",
      "100%|██████████| 3974/3974 [00:00<00:00, 4545449.71it/s]\n",
      "100%|██████████| 3974/3974 [00:00<00:00, 4593046.04it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3974, 300, 1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape up the training set\n",
    "import numpy as np\n",
    "\n",
    "dimensions = 300\n",
    "\n",
    "cores = multiprocessing.cpu_count()\n",
    "model_ug_dmm = Doc2Vec(dm=1, dm_mean=1, size=dimensions, window=4, negative=5, min_count=2, workers=cores, alpha=0.065, min_alpha=0.065)\n",
    "model_ug_dmm.build_vocab([x for x in tqdm(all_x_w2v)])\n",
    "\n",
    "for epoch in range(50):\n",
    "    model_ug_dmm.train(utils.shuffle([x for x in tqdm(all_x_w2v)]), total_examples=len(all_x_w2v), epochs=1)\n",
    "    model_ug_dmm.alpha -= 0.002\n",
    "    model_ug_dmm.min_alpha = model_ug_dmm.alpha\n",
    "    \n",
    "train_vecs_dmm = get_vectors(model_ug_dmm, cleaned, dimensions)\n",
    "# reshape a few things\n",
    "# https://stackoverflow.com/questions/46197493/using-gensim-doc2vec-with-keras-conv1d-valueerror\n",
    "X = np.array(train_vecs_dmm)\n",
    "\n",
    "X = X.reshape((X.shape[0],X.shape[1],1))\n",
    "\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape up the labels\n",
    "from sklearn import preprocessing\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit([True,False])\n",
    "Y = le.transform(tweets_only['isTrump'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2980 samples, validate on 994 samples\n",
      "Epoch 1/20\n",
      "2980/2980 [==============================] - 1s 217us/step - loss: 0.6759 - acc: 0.6205 - val_loss: 0.6656 - val_acc: 0.5885\n",
      "Epoch 2/20\n",
      "2980/2980 [==============================] - 0s 120us/step - loss: 0.6150 - acc: 0.6634 - val_loss: 0.6890 - val_acc: 0.5141\n",
      "Epoch 3/20\n",
      "2980/2980 [==============================] - 0s 122us/step - loss: 0.5934 - acc: 0.6789 - val_loss: 0.6228 - val_acc: 0.6781\n",
      "Epoch 4/20\n",
      "2980/2980 [==============================] - 0s 122us/step - loss: 0.6099 - acc: 0.6658 - val_loss: 0.6748 - val_acc: 0.5312\n",
      "Epoch 5/20\n",
      "2980/2980 [==============================] - 0s 123us/step - loss: 0.5994 - acc: 0.6738 - val_loss: 0.6715 - val_acc: 0.5513\n",
      "Epoch 6/20\n",
      "2980/2980 [==============================] - 0s 126us/step - loss: 0.5881 - acc: 0.6802 - val_loss: 0.6290 - val_acc: 0.6559\n",
      "Epoch 7/20\n",
      "2980/2980 [==============================] - 0s 123us/step - loss: 0.5960 - acc: 0.6846 - val_loss: 0.6308 - val_acc: 0.6579\n",
      "Epoch 8/20\n",
      "2980/2980 [==============================] - 0s 126us/step - loss: 0.5961 - acc: 0.6775 - val_loss: 0.6354 - val_acc: 0.6358\n",
      "Epoch 9/20\n",
      "2980/2980 [==============================] - 0s 127us/step - loss: 0.5882 - acc: 0.6819 - val_loss: 0.6381 - val_acc: 0.6338\n",
      "Epoch 10/20\n",
      "2980/2980 [==============================] - 0s 128us/step - loss: 0.5872 - acc: 0.6896 - val_loss: 0.6490 - val_acc: 0.5976\n",
      "Epoch 11/20\n",
      "2980/2980 [==============================] - 0s 127us/step - loss: 0.5858 - acc: 0.6899 - val_loss: 0.6701 - val_acc: 0.5483\n",
      "Epoch 12/20\n",
      "2980/2980 [==============================] - 0s 125us/step - loss: 0.5890 - acc: 0.6785 - val_loss: 0.6488 - val_acc: 0.5976\n",
      "Epoch 13/20\n",
      "2980/2980 [==============================] - 0s 126us/step - loss: 0.5826 - acc: 0.6919 - val_loss: 0.6446 - val_acc: 0.6056\n",
      "Epoch 14/20\n",
      "2980/2980 [==============================] - 0s 124us/step - loss: 0.5832 - acc: 0.6913 - val_loss: 0.6618 - val_acc: 0.5533\n",
      "Epoch 15/20\n",
      "2980/2980 [==============================] - 0s 124us/step - loss: 0.5877 - acc: 0.6799 - val_loss: 0.6257 - val_acc: 0.6579\n",
      "Epoch 16/20\n",
      "2980/2980 [==============================] - 0s 125us/step - loss: 0.5847 - acc: 0.6812 - val_loss: 0.6359 - val_acc: 0.6308\n",
      "Epoch 17/20\n",
      "2980/2980 [==============================] - 0s 125us/step - loss: 0.5881 - acc: 0.6768 - val_loss: 0.6235 - val_acc: 0.6700\n",
      "Epoch 18/20\n",
      "2980/2980 [==============================] - 0s 124us/step - loss: 0.5846 - acc: 0.6872 - val_loss: 0.6309 - val_acc: 0.6499\n",
      "Epoch 19/20\n",
      "2980/2980 [==============================] - 0s 125us/step - loss: 0.5844 - acc: 0.6859 - val_loss: 0.6695 - val_acc: 0.5402\n",
      "Epoch 20/20\n",
      "2980/2980 [==============================] - 0s 124us/step - loss: 0.5794 - acc: 0.6872 - val_loss: 0.6710 - val_acc: 0.5382\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0cecb89dd8>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now that we have the vectors, we can try to classify. \n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, MaxPooling1D\n",
    "\n",
    "# set parameters:\n",
    "batch_size = 128\n",
    "filters = 64\n",
    "kernel_size = 3\n",
    "hidden_dims = 250\n",
    "epochs = 20\n",
    "\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# we start off with an efficient embedding layer which maps\n",
    "# our vocab indices into embedding_dims dimensions\n",
    "#model.add(Embedding(max_features,\n",
    "#                    embedding_dims,\n",
    "#                    input_length=maxlen))\n",
    "\n",
    "\n",
    "#model.add(Dropout(0.2))\n",
    "\n",
    "# we add a Convolution1D, which will learn filters\n",
    "# word group filters of size filter_length:\n",
    "model.add(Conv1D(input_shape = (X.shape[1],X.shape[2]),\n",
    "                 filters=filters,\n",
    "                 kernel_size=kernel_size,\n",
    "                 padding='valid',\n",
    "                 activation='relu'))\n",
    "model.add(Conv1D(filters=filters,\n",
    "                 kernel_size=kernel_size, activation='relu'))\n",
    "model.add(Conv1D(filters=filters,\n",
    "                 kernel_size=kernel_size, activation='relu'))\n",
    "model.add(Conv1D(filters=filters,\n",
    "                 kernel_size=kernel_size, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# Use max pooling:\n",
    "model.add(MaxPooling1D(3))\n",
    "\n",
    "model.add(Conv1D(filters=filters,\n",
    "                 kernel_size=2, activation='relu'))\n",
    "model.add(Conv1D(filters=filters,\n",
    "                 kernel_size=2, activation='relu'))\n",
    "model.add(Conv1D(filters=filters,\n",
    "                 kernel_size=2, activation='relu'))\n",
    "model.add(Conv1D(filters=filters,\n",
    "                 kernel_size=2, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# Use max pooling:\n",
    "model.add(GlobalMaxPooling1D())\n",
    "\n",
    "# Add a vanilla hidden layer:\n",
    "model.add(Dense(hidden_dims))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X, Y,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.25)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ug_dmm.most_similar(\"maga\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
