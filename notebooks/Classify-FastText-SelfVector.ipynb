{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../src/data/raw_tweets.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  data['isTrump'] = [True if x == 'Twitter for Android' else False for x in data['source']]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "from data import raw_tweets\n",
    "\n",
    "all_data = raw_tweets.load()\n",
    "all_data = raw_tweets.label(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_only = all_data[all_data.is_retweet ==False]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/keras/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from features import build_features\n",
    "    \n",
    "cleaned = tweets_only['text'].map(build_features.clean_text)\n",
    "   \n",
    "cleaned.to_csv('../data/processed/just-tweets.txt', index=False, sep='\\t', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do some fasttexting\n",
    "import fastText\n",
    "\n",
    "max_features = 100\n",
    "\n",
    "ft_model = fastText.FastText.train_unsupervised('../data/processed/just-tweets.txt',\n",
    "                                                wordNgrams=3,\n",
    "                                                thread=4,\n",
    "                                                model='cbow',\n",
    "                                                dim = max_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3974, 100, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape up the training set\n",
    "import numpy as np\n",
    "\n",
    "X = []\n",
    "for text in cleaned:\n",
    "    vec = ft_model.get_sentence_vector(text)\n",
    "    X.append(vec)\n",
    "\n",
    "# reshape a few things\n",
    "# https://stackoverflow.com/questions/46197493/using-gensim-doc2vec-with-keras-conv1d-valueerror\n",
    "X = np.array(X)\n",
    "\n",
    "X = X.reshape((X.shape[0],X.shape[1],1))\n",
    "\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape up the labels\n",
    "from sklearn import preprocessing\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit([True,False])\n",
    "Y = le.transform(tweets_only['isTrump'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3576 samples, validate on 398 samples\n",
      "Epoch 1/20\n",
      "3576/3576 [==============================] - 2s 521us/step - loss: 0.6907 - acc: 0.5405 - val_loss: 0.6944 - val_acc: 0.5126\n",
      "Epoch 2/20\n",
      "3576/3576 [==============================] - 0s 30us/step - loss: 0.6907 - acc: 0.5414 - val_loss: 0.6931 - val_acc: 0.5126\n",
      "Epoch 3/20\n",
      "3576/3576 [==============================] - 0s 30us/step - loss: 0.6903 - acc: 0.5417 - val_loss: 0.6963 - val_acc: 0.5126\n",
      "Epoch 4/20\n",
      "3576/3576 [==============================] - 0s 29us/step - loss: 0.6902 - acc: 0.5414 - val_loss: 0.6935 - val_acc: 0.5126\n",
      "Epoch 5/20\n",
      "3576/3576 [==============================] - 0s 29us/step - loss: 0.6902 - acc: 0.5414 - val_loss: 0.6938 - val_acc: 0.5126\n",
      "Epoch 6/20\n",
      "3576/3576 [==============================] - 0s 30us/step - loss: 0.6901 - acc: 0.5414 - val_loss: 0.6939 - val_acc: 0.5126\n",
      "Epoch 7/20\n",
      "3576/3576 [==============================] - 0s 30us/step - loss: 0.6897 - acc: 0.5414 - val_loss: 0.6934 - val_acc: 0.5126\n",
      "Epoch 8/20\n",
      "3576/3576 [==============================] - 0s 30us/step - loss: 0.6902 - acc: 0.5408 - val_loss: 0.6958 - val_acc: 0.5126\n",
      "Epoch 9/20\n",
      "3576/3576 [==============================] - 0s 30us/step - loss: 0.6904 - acc: 0.5414 - val_loss: 0.6937 - val_acc: 0.5126\n",
      "Epoch 10/20\n",
      "3576/3576 [==============================] - 0s 29us/step - loss: 0.6900 - acc: 0.5414 - val_loss: 0.6941 - val_acc: 0.5126\n",
      "Epoch 11/20\n",
      "3576/3576 [==============================] - 0s 30us/step - loss: 0.6901 - acc: 0.5414 - val_loss: 0.6942 - val_acc: 0.5126\n",
      "Epoch 12/20\n",
      "3576/3576 [==============================] - 0s 31us/step - loss: 0.6901 - acc: 0.5414 - val_loss: 0.6946 - val_acc: 0.5126\n",
      "Epoch 13/20\n",
      "3576/3576 [==============================] - 0s 30us/step - loss: 0.6902 - acc: 0.5414 - val_loss: 0.6938 - val_acc: 0.5126\n",
      "Epoch 14/20\n",
      "3576/3576 [==============================] - 0s 29us/step - loss: 0.6901 - acc: 0.5414 - val_loss: 0.6950 - val_acc: 0.5126\n",
      "Epoch 15/20\n",
      "3576/3576 [==============================] - 0s 30us/step - loss: 0.6899 - acc: 0.5414 - val_loss: 0.6946 - val_acc: 0.5126\n",
      "Epoch 16/20\n",
      "3576/3576 [==============================] - 0s 30us/step - loss: 0.6901 - acc: 0.5414 - val_loss: 0.6937 - val_acc: 0.5126\n",
      "Epoch 17/20\n",
      "3576/3576 [==============================] - 0s 30us/step - loss: 0.6901 - acc: 0.5414 - val_loss: 0.6947 - val_acc: 0.5126\n",
      "Epoch 18/20\n",
      "3576/3576 [==============================] - 0s 30us/step - loss: 0.6902 - acc: 0.5414 - val_loss: 0.6946 - val_acc: 0.5126\n",
      "Epoch 19/20\n",
      "3576/3576 [==============================] - 0s 30us/step - loss: 0.6901 - acc: 0.5414 - val_loss: 0.6936 - val_acc: 0.5126\n",
      "Epoch 20/20\n",
      "3576/3576 [==============================] - 0s 30us/step - loss: 0.6897 - acc: 0.5414 - val_loss: 0.6949 - val_acc: 0.5126\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7efc54a0efd0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now that we have the vectors, we can try to classify. \n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "\n",
    "# set parameters:\n",
    "batch_size = 128\n",
    "filters = 250\n",
    "kernel_size = 2\n",
    "hidden_dims = 250\n",
    "epochs = 20\n",
    "\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# we start off with an efficient embedding layer which maps\n",
    "# our vocab indices into embedding_dims dimensions\n",
    "#model.add(Embedding(max_features,\n",
    "#                    embedding_dims,\n",
    "#                    input_length=maxlen))\n",
    "\n",
    "\n",
    "#model.add(Dropout(0.2))\n",
    "\n",
    "# we add a Convolution1D, which will learn filters\n",
    "# word group filters of size filter_length:\n",
    "model.add(Conv1D(input_shape = (X.shape[1],X.shape[2]),\n",
    "                 filters=filters,\n",
    "                 kernel_size=kernel_size,\n",
    "                 padding='valid',\n",
    "                 activation='relu'))\n",
    "\n",
    "# Use max pooling:\n",
    "model.add(GlobalMaxPooling1D())\n",
    "\n",
    "# Add a vanilla hidden layer:\n",
    "model.add(Dense(hidden_dims))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# Add a vanilla hidden layer:\n",
    "model.add(Dense(50))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X, Y,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, ..., 0, 0, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.02562395],\n",
       "        [ 0.12267342],\n",
       "        [-0.22090374],\n",
       "        ...,\n",
       "        [-0.03441325],\n",
       "        [ 0.01117426],\n",
       "        [-0.13769613]],\n",
       "\n",
       "       [[ 0.02631074],\n",
       "        [ 0.12256388],\n",
       "        [-0.22172602],\n",
       "        ...,\n",
       "        [-0.03418927],\n",
       "        [ 0.01164399],\n",
       "        [-0.13741669]],\n",
       "\n",
       "       [[ 0.02632573],\n",
       "        [ 0.12237485],\n",
       "        [-0.22180703],\n",
       "        ...,\n",
       "        [-0.03365884],\n",
       "        [ 0.01159522],\n",
       "        [-0.13782223]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 0.02674141],\n",
       "        [ 0.12294166],\n",
       "        [-0.22161585],\n",
       "        ...,\n",
       "        [-0.0347605 ],\n",
       "        [ 0.01071497],\n",
       "        [-0.13757446]],\n",
       "\n",
       "       [[ 0.02612884],\n",
       "        [ 0.12224888],\n",
       "        [-0.22117734],\n",
       "        ...,\n",
       "        [-0.03427832],\n",
       "        [ 0.01144536],\n",
       "        [-0.13772956]],\n",
       "\n",
       "       [[ 0.0253894 ],\n",
       "        [ 0.12241374],\n",
       "        [-0.22168326],\n",
       "        ...,\n",
       "        [-0.03380027],\n",
       "        [ 0.01163926],\n",
       "        [-0.1375364 ]]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module fastText.FastText in fastText:\n",
      "\n",
      "NAME\n",
      "    fastText.FastText\n",
      "\n",
      "DESCRIPTION\n",
      "    # Copyright (c) 2017-present, Facebook, Inc.\n",
      "    # All rights reserved.\n",
      "    #\n",
      "    # This source code is licensed under the BSD-style license found in the\n",
      "    # LICENSE file in the root directory of this source tree. An additional grant\n",
      "    # of patent rights can be found in the PATENTS file in the same directory.\n",
      "\n",
      "FUNCTIONS\n",
      "    load_model(path)\n",
      "        Load a model given a filepath and return a model object.\n",
      "    \n",
      "    tokenize(text)\n",
      "        Given a string of text, tokenize it and return a list of tokens\n",
      "    \n",
      "    train_supervised(input, lr=0.1, dim=100, ws=5, epoch=5, minCount=1, minCountLabel=0, minn=0, maxn=0, neg=5, wordNgrams=1, loss='softmax', bucket=2000000, thread=12, lrUpdateRate=100, t=0.0001, label='__label__', verbose=2, pretrainedVectors='')\n",
      "        Train a supervised model and return a model object.\n",
      "        \n",
      "        input must be a filepath. The input text does not need to be tokenized\n",
      "        as per the tokenize function, but it must be preprocessed and encoded\n",
      "        as UTF-8. You might want to consult standard preprocessing scripts such\n",
      "        as tokenizer.perl mentioned here: http://www.statmt.org/wmt07/baseline.html\n",
      "        \n",
      "        The input file must must contain at least one label per line. For an\n",
      "        example consult the example datasets which are part of the fastText\n",
      "        repository such as the dataset pulled by classification-example.sh.\n",
      "    \n",
      "    train_unsupervised(input, model='skipgram', lr=0.05, dim=100, ws=5, epoch=5, minCount=5, minCountLabel=0, minn=3, maxn=6, neg=5, wordNgrams=1, loss='ns', bucket=2000000, thread=12, lrUpdateRate=100, t=0.0001, label='__label__', verbose=2, pretrainedVectors='')\n",
      "        Train an unsupervised model and return a model object.\n",
      "        \n",
      "        input must be a filepath. The input text does not need to be tokenized\n",
      "        as per the tokenize function, but it must be preprocessed and encoded\n",
      "        as UTF-8. You might want to consult standard preprocessing scripts such\n",
      "        as tokenizer.perl mentioned here: http://www.statmt.org/wmt07/baseline.html\n",
      "        \n",
      "        The input field must not contain any labels or use the specified label prefix\n",
      "        unless it is ok for those words to be ignored. For an example consult the\n",
      "        dataset pulled by the example script word-vector-example.sh, which is\n",
      "        part of the fastText repository.\n",
      "\n",
      "DATA\n",
      "    BOW = '<'\n",
      "    EOS = '</s>'\n",
      "    EOW = '>'\n",
      "    absolute_import = _Feature((2, 5, 0, 'alpha', 1), (3, 0, 0, 'alpha', 0...\n",
      "    division = _Feature((2, 2, 0, 'alpha', 2), (3, 0, 0, 'alpha', 0), 8192...\n",
      "    print_function = _Feature((2, 6, 0, 'alpha', 2), (3, 0, 0, 'alpha', 0)...\n",
      "    unicode_literals = _Feature((2, 6, 0, 'alpha', 2), (3, 0, 0, 'alpha', ...\n",
      "\n",
      "FILE\n",
      "    /opt/conda/lib/python3.5/site-packages/fastText/FastText.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(fastText.FastText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastText.util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package fastText.util in fastText:\n",
      "\n",
      "NAME\n",
      "    fastText.util\n",
      "\n",
      "DESCRIPTION\n",
      "    # Copyright (c) 2017-present, Facebook, Inc.\n",
      "    # All rights reserved.\n",
      "    #\n",
      "    # This source code is licensed under the BSD-style license found in the\n",
      "    # LICENSE file in the root directory of this source tree. An additional grant\n",
      "    # of patent rights can be found in the PATENTS file in the same directory.\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    util\n",
      "\n",
      "DATA\n",
      "    absolute_import = _Feature((2, 5, 0, 'alpha', 1), (3, 0, 0, 'alpha', 0...\n",
      "    division = _Feature((2, 2, 0, 'alpha', 2), (3, 0, 0, 'alpha', 0), 8192...\n",
      "    print_function = _Feature((2, 6, 0, 'alpha', 2), (3, 0, 0, 'alpha', 0)...\n",
      "    unicode_literals = _Feature((2, 6, 0, 'alpha', 2), (3, 0, 0, 'alpha', ...\n",
      "\n",
      "FILE\n",
      "    /opt/conda/lib/python3.5/site-packages/fastText/util/__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(fastText.util)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Object arrays are not currently supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-8394c7281988>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfastText\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_nearest_neighbor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CrookedHillary\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mft_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mban_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/fastText/util/util.py\u001b[0m in \u001b[0;36mfind_nearest_neighbor\u001b[0;34m(query, vectors, ban_set, cossims)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \"\"\"\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcossims\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mcossims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcossims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcossims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Object arrays are not currently supported"
     ]
    }
   ],
   "source": [
    "fastText.util.find_nearest_neighbor(\"CrookedHillary\",ft_model, ban_set='' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(ft_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
