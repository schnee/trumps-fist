{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>id_str</th>\n",
       "      <th>in_reply_to_user_id_str</th>\n",
       "      <th>is_retweet</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>isTrump</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>2327</td>\n",
       "      <td>2327</td>\n",
       "      <td>2327</td>\n",
       "      <td>2</td>\n",
       "      <td>2327</td>\n",
       "      <td>2327</td>\n",
       "      <td>2327</td>\n",
       "      <td>2327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>1835</td>\n",
       "      <td>1835</td>\n",
       "      <td>1835</td>\n",
       "      <td>4</td>\n",
       "      <td>1835</td>\n",
       "      <td>1835</td>\n",
       "      <td>1835</td>\n",
       "      <td>1835</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         created_at  favorite_count  id_str  in_reply_to_user_id_str  \\\n",
       "isTrump                                                                \n",
       "False          2327            2327    2327                        2   \n",
       "True           1835            1835    1835                        4   \n",
       "\n",
       "         is_retweet  retweet_count  source  text  \n",
       "isTrump                                           \n",
       "False          2327           2327    2327  2327  \n",
       "True           1835           1835    1835  1835  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "stop_words = pickle.load(open(\"../data/external/stop_words_en.pkl\", \"rb\"))\n",
    "TRAIN_FILENAME = \"../data/external/condensed_2016.json.zip\"\n",
    "\n",
    "all_data = pd.read_json(TRAIN_FILENAME)\n",
    "\n",
    "all_data = all_data.loc[all_data['source'] != 'Twitter Ads']\n",
    "\n",
    "# Assume that only Android is authored by Trump. All else is not Trump.\n",
    "def isTrump(row):\n",
    "    if row['source'] == 'Twitter for Android' :\n",
    "      return True\n",
    "    return False\n",
    "\n",
    "all_data['isTrump'] = all_data.apply(lambda row: isTrump (row),axis=1)\n",
    "\n",
    "all_data.groupby('isTrump').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_only = all_data[all_data.is_retweet ==False]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list of contractions from http://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\n",
    "contractions = { \n",
    "\"ain't\": \"am not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"needn't\": \"need not\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there had\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who's\": \"who is\",\n",
    "\"won't\": \"will not\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you're\": \"you are\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_text(text, remove_stopwords = True):\n",
    "    '''Remove unwanted characters, stopwords, and format the text to create fewer nulls word embeddings'''\n",
    "    \n",
    "    # Convert words to lower case\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Replace contractions with their longer forms \n",
    "    if True:\n",
    "        text = text.split()\n",
    "        new_text = []\n",
    "        for word in text:\n",
    "            if word in contractions:\n",
    "                new_text.append(contractions[word])\n",
    "            else:\n",
    "                new_text.append(word)\n",
    "        text = \" \".join(new_text)\n",
    "    \n",
    "    # Format words and remove unwanted characters\n",
    "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\<a href', ' ', text)\n",
    "    text = re.sub(r'&amp;', '', text) \n",
    "    text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n",
    "    text = re.sub(r'<br />', ' ', text)\n",
    "    text = re.sub(r'\\'', ' ', text)\n",
    "    text = re.sub(r'\"', ' ', text)\n",
    "    text = re.sub('\\n',' ',text)\n",
    "    \n",
    "    # Optionally, remove stop words\n",
    "    if remove_stopwords:\n",
    "        text = text.split()\n",
    "        #stops = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stop_words]\n",
    "        text = \" \".join(text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweet(line):\n",
    "    line = line.lower()\n",
    "    line = re.sub('\"','',line)\n",
    "    line = re.sub('\\'','',line)\n",
    "    line = re.sub('\\n',' ',line)\n",
    "    line = re.sub('\\t',' ',line)\n",
    "    return line\n",
    "    \n",
    "cleaned = tweets_only['text'].map(clean_text)\n",
    "   \n",
    "cleaned.to_csv('../data/processed/just-tweets.txt', index=False, sep='\\t', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do some fasttexting\n",
    "import fastText\n",
    "\n",
    "max_features = 100\n",
    "\n",
    "ft_model = fastText.FastText.train_unsupervised('../data/processed/just-tweets.txt',\n",
    "                                                wordNgrams=3,\n",
    "                                                thread=4,\n",
    "                                                model='cbow',\n",
    "                                                dim = max_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3974, 100, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape up the training set\n",
    "import numpy as np\n",
    "\n",
    "X = []\n",
    "for text in cleaned:\n",
    "    vec = ft_model.get_sentence_vector(text)\n",
    "    X.append(vec)\n",
    "\n",
    "# reshape a few things\n",
    "# https://stackoverflow.com/questions/46197493/using-gensim-doc2vec-with-keras-conv1d-valueerror\n",
    "X = np.array(X)\n",
    "\n",
    "X = X.reshape((X.shape[0],X.shape[1],1))\n",
    "\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape up the labels\n",
    "from sklearn import preprocessing\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit([True,False])\n",
    "Y = le.transform(tweets_only['isTrump'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3576 samples, validate on 398 samples\n",
      "Epoch 1/20\n",
      "3576/3576 [==============================] - 2s 518us/step - loss: 0.6905 - acc: 0.5372 - val_loss: 0.6950 - val_acc: 0.5126\n",
      "Epoch 2/20\n",
      "3576/3576 [==============================] - 0s 30us/step - loss: 0.6903 - acc: 0.5419 - val_loss: 0.6947 - val_acc: 0.5126\n",
      "Epoch 3/20\n",
      "3576/3576 [==============================] - 0s 30us/step - loss: 0.6902 - acc: 0.5411 - val_loss: 0.6960 - val_acc: 0.5126\n",
      "Epoch 4/20\n",
      "3576/3576 [==============================] - 0s 29us/step - loss: 0.6905 - acc: 0.5414 - val_loss: 0.6943 - val_acc: 0.5126\n",
      "Epoch 5/20\n",
      "3576/3576 [==============================] - 0s 30us/step - loss: 0.6904 - acc: 0.5411 - val_loss: 0.6942 - val_acc: 0.5126\n",
      "Epoch 6/20\n",
      "3576/3576 [==============================] - 0s 30us/step - loss: 0.6905 - acc: 0.5414 - val_loss: 0.6939 - val_acc: 0.5126\n",
      "Epoch 7/20\n",
      "3576/3576 [==============================] - 0s 29us/step - loss: 0.6900 - acc: 0.5414 - val_loss: 0.6947 - val_acc: 0.5126\n",
      "Epoch 8/20\n",
      "3576/3576 [==============================] - 0s 30us/step - loss: 0.6907 - acc: 0.5414 - val_loss: 0.6944 - val_acc: 0.5126\n",
      "Epoch 9/20\n",
      "3576/3576 [==============================] - 0s 29us/step - loss: 0.6900 - acc: 0.5414 - val_loss: 0.6943 - val_acc: 0.5126\n",
      "Epoch 10/20\n",
      "3576/3576 [==============================] - 0s 30us/step - loss: 0.6901 - acc: 0.5414 - val_loss: 0.6941 - val_acc: 0.5126\n",
      "Epoch 11/20\n",
      "3576/3576 [==============================] - 0s 30us/step - loss: 0.6902 - acc: 0.5414 - val_loss: 0.6938 - val_acc: 0.5126\n",
      "Epoch 12/20\n",
      "3576/3576 [==============================] - 0s 31us/step - loss: 0.6903 - acc: 0.5414 - val_loss: 0.6940 - val_acc: 0.5126\n",
      "Epoch 13/20\n",
      "3576/3576 [==============================] - 0s 30us/step - loss: 0.6903 - acc: 0.5414 - val_loss: 0.6944 - val_acc: 0.5126\n",
      "Epoch 14/20\n",
      "3576/3576 [==============================] - 0s 29us/step - loss: 0.6902 - acc: 0.5414 - val_loss: 0.6942 - val_acc: 0.5126\n",
      "Epoch 15/20\n",
      "3576/3576 [==============================] - 0s 29us/step - loss: 0.6902 - acc: 0.5414 - val_loss: 0.6943 - val_acc: 0.5126\n",
      "Epoch 16/20\n",
      "3576/3576 [==============================] - 0s 29us/step - loss: 0.6898 - acc: 0.5414 - val_loss: 0.6945 - val_acc: 0.5126\n",
      "Epoch 17/20\n",
      "3576/3576 [==============================] - 0s 31us/step - loss: 0.6901 - acc: 0.5414 - val_loss: 0.6939 - val_acc: 0.5126\n",
      "Epoch 18/20\n",
      "3576/3576 [==============================] - 0s 30us/step - loss: 0.6895 - acc: 0.5414 - val_loss: 0.6951 - val_acc: 0.5126\n",
      "Epoch 19/20\n",
      "3576/3576 [==============================] - 0s 30us/step - loss: 0.6899 - acc: 0.5414 - val_loss: 0.6947 - val_acc: 0.5126\n",
      "Epoch 20/20\n",
      "3576/3576 [==============================] - 0s 30us/step - loss: 0.6897 - acc: 0.5414 - val_loss: 0.6949 - val_acc: 0.5126\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb7ef464b70>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now that we have the vectors, we can try to classify. \n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "\n",
    "# set parameters:\n",
    "batch_size = 128\n",
    "filters = 250\n",
    "kernel_size = 2\n",
    "hidden_dims = 250\n",
    "epochs = 20\n",
    "\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# we start off with an efficient embedding layer which maps\n",
    "# our vocab indices into embedding_dims dimensions\n",
    "#model.add(Embedding(max_features,\n",
    "#                    embedding_dims,\n",
    "#                    input_length=maxlen))\n",
    "\n",
    "\n",
    "#model.add(Dropout(0.2))\n",
    "\n",
    "# we add a Convolution1D, which will learn filters\n",
    "# word group filters of size filter_length:\n",
    "model.add(Conv1D(input_shape = (X.shape[1],X.shape[2]),\n",
    "                 filters=filters,\n",
    "                 kernel_size=kernel_size,\n",
    "                 padding='valid',\n",
    "                 activation='relu'))\n",
    "\n",
    "# Use max pooling:\n",
    "model.add(GlobalMaxPooling1D())\n",
    "\n",
    "# Add a vanilla hidden layer:\n",
    "model.add(Dense(hidden_dims))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# Add a vanilla hidden layer:\n",
    "model.add(Dense(50))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X, Y,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, ..., 0, 0, 1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.08728243],\n",
       "        [-0.1128547 ],\n",
       "        [-0.00620146],\n",
       "        ...,\n",
       "        [-0.11858591],\n",
       "        [-0.02351624],\n",
       "        [ 0.00816149]],\n",
       "\n",
       "       [[ 0.08779234],\n",
       "        [-0.11284603],\n",
       "        [-0.00658891],\n",
       "        ...,\n",
       "        [-0.11880445],\n",
       "        [-0.0235959 ],\n",
       "        [ 0.00803726]],\n",
       "\n",
       "       [[ 0.0875293 ],\n",
       "        [-0.1128301 ],\n",
       "        [-0.00718569],\n",
       "        ...,\n",
       "        [-0.11879747],\n",
       "        [-0.02352853],\n",
       "        [ 0.0078991 ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 0.08820534],\n",
       "        [-0.11232013],\n",
       "        [-0.00707519],\n",
       "        ...,\n",
       "        [-0.11911884],\n",
       "        [-0.02436996],\n",
       "        [ 0.00731871]],\n",
       "\n",
       "       [[ 0.08758877],\n",
       "        [-0.11262521],\n",
       "        [-0.00678988],\n",
       "        ...,\n",
       "        [-0.11877411],\n",
       "        [-0.02384715],\n",
       "        [ 0.00831338]],\n",
       "\n",
       "       [[ 0.08845446],\n",
       "        [-0.1132536 ],\n",
       "        [-0.00601076],\n",
       "        ...,\n",
       "        [-0.11878899],\n",
       "        [-0.0235492 ],\n",
       "        [ 0.00741486]]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module fastText.FastText in fastText:\n",
      "\n",
      "NAME\n",
      "    fastText.FastText\n",
      "\n",
      "DESCRIPTION\n",
      "    # Copyright (c) 2017-present, Facebook, Inc.\n",
      "    # All rights reserved.\n",
      "    #\n",
      "    # This source code is licensed under the BSD-style license found in the\n",
      "    # LICENSE file in the root directory of this source tree. An additional grant\n",
      "    # of patent rights can be found in the PATENTS file in the same directory.\n",
      "\n",
      "FUNCTIONS\n",
      "    load_model(path)\n",
      "        Load a model given a filepath and return a model object.\n",
      "    \n",
      "    tokenize(text)\n",
      "        Given a string of text, tokenize it and return a list of tokens\n",
      "    \n",
      "    train_supervised(input, lr=0.1, dim=100, ws=5, epoch=5, minCount=1, minCountLabel=0, minn=0, maxn=0, neg=5, wordNgrams=1, loss='softmax', bucket=2000000, thread=12, lrUpdateRate=100, t=0.0001, label='__label__', verbose=2, pretrainedVectors='')\n",
      "        Train a supervised model and return a model object.\n",
      "        \n",
      "        input must be a filepath. The input text does not need to be tokenized\n",
      "        as per the tokenize function, but it must be preprocessed and encoded\n",
      "        as UTF-8. You might want to consult standard preprocessing scripts such\n",
      "        as tokenizer.perl mentioned here: http://www.statmt.org/wmt07/baseline.html\n",
      "        \n",
      "        The input file must must contain at least one label per line. For an\n",
      "        example consult the example datasets which are part of the fastText\n",
      "        repository such as the dataset pulled by classification-example.sh.\n",
      "    \n",
      "    train_unsupervised(input, model='skipgram', lr=0.05, dim=100, ws=5, epoch=5, minCount=5, minCountLabel=0, minn=3, maxn=6, neg=5, wordNgrams=1, loss='ns', bucket=2000000, thread=12, lrUpdateRate=100, t=0.0001, label='__label__', verbose=2, pretrainedVectors='')\n",
      "        Train an unsupervised model and return a model object.\n",
      "        \n",
      "        input must be a filepath. The input text does not need to be tokenized\n",
      "        as per the tokenize function, but it must be preprocessed and encoded\n",
      "        as UTF-8. You might want to consult standard preprocessing scripts such\n",
      "        as tokenizer.perl mentioned here: http://www.statmt.org/wmt07/baseline.html\n",
      "        \n",
      "        The input field must not contain any labels or use the specified label prefix\n",
      "        unless it is ok for those words to be ignored. For an example consult the\n",
      "        dataset pulled by the example script word-vector-example.sh, which is\n",
      "        part of the fastText repository.\n",
      "\n",
      "DATA\n",
      "    BOW = '<'\n",
      "    EOS = '</s>'\n",
      "    EOW = '>'\n",
      "    absolute_import = _Feature((2, 5, 0, 'alpha', 1), (3, 0, 0, 'alpha', 0...\n",
      "    division = _Feature((2, 2, 0, 'alpha', 2), (3, 0, 0, 'alpha', 0), 8192...\n",
      "    print_function = _Feature((2, 6, 0, 'alpha', 2), (3, 0, 0, 'alpha', 0)...\n",
      "    unicode_literals = _Feature((2, 6, 0, 'alpha', 2), (3, 0, 0, 'alpha', ...\n",
      "\n",
      "FILE\n",
      "    /opt/conda/lib/python3.5/site-packages/fastText/FastText.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(fastText.FastText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastText.util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package fastText.util in fastText:\n",
      "\n",
      "NAME\n",
      "    fastText.util\n",
      "\n",
      "DESCRIPTION\n",
      "    # Copyright (c) 2017-present, Facebook, Inc.\n",
      "    # All rights reserved.\n",
      "    #\n",
      "    # This source code is licensed under the BSD-style license found in the\n",
      "    # LICENSE file in the root directory of this source tree. An additional grant\n",
      "    # of patent rights can be found in the PATENTS file in the same directory.\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    util\n",
      "\n",
      "DATA\n",
      "    absolute_import = _Feature((2, 5, 0, 'alpha', 1), (3, 0, 0, 'alpha', 0...\n",
      "    division = _Feature((2, 2, 0, 'alpha', 2), (3, 0, 0, 'alpha', 0), 8192...\n",
      "    print_function = _Feature((2, 6, 0, 'alpha', 2), (3, 0, 0, 'alpha', 0)...\n",
      "    unicode_literals = _Feature((2, 6, 0, 'alpha', 2), (3, 0, 0, 'alpha', ...\n",
      "\n",
      "FILE\n",
      "    /opt/conda/lib/python3.5/site-packages/fastText/util/__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(fastText.util)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Object arrays are not currently supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-8394c7281988>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfastText\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_nearest_neighbor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CrookedHillary\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mft_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mban_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/fastText/util/util.py\u001b[0m in \u001b[0;36mfind_nearest_neighbor\u001b[0;34m(query, vectors, ban_set, cossims)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \"\"\"\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcossims\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mcossims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcossims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcossims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Object arrays are not currently supported"
     ]
    }
   ],
   "source": [
    "fastText.util.find_nearest_neighbor(\"CrookedHillary\",ft_model, ban_set='' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(ft_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
