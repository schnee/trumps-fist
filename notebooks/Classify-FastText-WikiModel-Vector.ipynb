{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>id_str</th>\n",
       "      <th>in_reply_to_user_id_str</th>\n",
       "      <th>is_retweet</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>isTrump</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>2327</td>\n",
       "      <td>2327</td>\n",
       "      <td>2327</td>\n",
       "      <td>2</td>\n",
       "      <td>2327</td>\n",
       "      <td>2327</td>\n",
       "      <td>2327</td>\n",
       "      <td>2327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>1835</td>\n",
       "      <td>1835</td>\n",
       "      <td>1835</td>\n",
       "      <td>4</td>\n",
       "      <td>1835</td>\n",
       "      <td>1835</td>\n",
       "      <td>1835</td>\n",
       "      <td>1835</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         created_at  favorite_count  id_str  in_reply_to_user_id_str  \\\n",
       "isTrump                                                                \n",
       "False          2327            2327    2327                        2   \n",
       "True           1835            1835    1835                        4   \n",
       "\n",
       "         is_retweet  retweet_count  source  text  \n",
       "isTrump                                           \n",
       "False          2327           2327    2327  2327  \n",
       "True           1835           1835    1835  1835  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "stop_words = pickle.load(open(\"../data/external/stop_words_en.pkl\", \"rb\"))\n",
    "TRAIN_FILENAME = \"/projects/github.com/trumps-fist/data/external/condensed_2016.json.zip\"\n",
    "\n",
    "all_data = pd.read_json(TRAIN_FILENAME)\n",
    "\n",
    "all_data = all_data.loc[all_data['source'] != 'Twitter Ads']\n",
    "\n",
    "# Assume that only Android is authored by Trump. All else is not Trump.\n",
    "def isTrump(row):\n",
    "    if row['source'] == 'Twitter for Android' :\n",
    "      return True\n",
    "    return False\n",
    "\n",
    "all_data['isTrump'] = all_data.apply(lambda row: isTrump (row),axis=1)\n",
    "\n",
    "all_data.groupby('isTrump').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_only = all_data[all_data.is_retweet ==False]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list of contractions from http://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\n",
    "contractions = { \n",
    "\"ain't\": \"am not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"needn't\": \"need not\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there had\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who's\": \"who is\",\n",
    "\"won't\": \"will not\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you're\": \"you are\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_text(text, remove_stopwords = True):\n",
    "    '''Remove unwanted characters, stopwords, and format the text to create fewer nulls word embeddings'''\n",
    "    \n",
    "    # Convert words to lower case\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Replace contractions with their longer forms \n",
    "    if True:\n",
    "        text = text.split()\n",
    "        new_text = []\n",
    "        for word in text:\n",
    "            if word in contractions:\n",
    "                new_text.append(contractions[word])\n",
    "            else:\n",
    "                new_text.append(word)\n",
    "        text = \" \".join(new_text)\n",
    "    \n",
    "    # Format words and remove unwanted characters\n",
    "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\<a href', ' ', text)\n",
    "    text = re.sub(r'&amp;', '', text) \n",
    "    text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n",
    "    text = re.sub(r'<br />', ' ', text)\n",
    "    text = re.sub(r'\\'', ' ', text)\n",
    "    text = re.sub(r'\"', ' ', text)\n",
    "    text = re.sub('\\n',' ',text)\n",
    "    \n",
    "    # Optionally, remove stop words\n",
    "    if remove_stopwords:\n",
    "        text = text.split()\n",
    "        #stops = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stop_words]\n",
    "        text = \" \".join(text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweet(line):\n",
    "    line = line.lower()\n",
    "    line = re.sub('\"','',line)\n",
    "    line = re.sub('\\'','',line)\n",
    "    line = re.sub('\\n',' ',line)\n",
    "    line = re.sub('\\t',' ',line)\n",
    "    return line\n",
    "    \n",
    "cleaned = tweets_only['text'].map(clean_text)\n",
    "   \n",
    "cleaned.to_csv('../data/processed/just-tweets.txt', index=False, sep='\\t', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load up a fasttext model from the pretrained wikipedia model\n",
    "# https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md\n",
    "import fastText\n",
    "\n",
    "\n",
    "ft_model = fastText.FastText.load_model('../data/external/wiki.en.bin')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3974, 300, 1)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape up the training set\n",
    "import numpy as np\n",
    "\n",
    "X = []\n",
    "for text in cleaned:\n",
    "    vec = ft_model.get_sentence_vector(text)\n",
    "    X.append(vec)\n",
    "\n",
    "# reshape a few things\n",
    "# https://stackoverflow.com/questions/46197493/using-gensim-doc2vec-with-keras-conv1d-valueerror\n",
    "X = np.array(X)\n",
    "\n",
    "X = X.reshape((X.shape[0],X.shape[1],1))\n",
    "\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape up the labels\n",
    "from sklearn import preprocessing\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit([True,False])\n",
    "Y = le.transform(tweets_only['isTrump'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3576 samples, validate on 398 samples\n",
      "Epoch 1/20\n",
      "3576/3576 [==============================] - 0s 134us/step - loss: 0.6907 - acc: 0.5397 - val_loss: 0.6948 - val_acc: 0.5126\n",
      "Epoch 2/20\n",
      "3576/3576 [==============================] - 0s 67us/step - loss: 0.6891 - acc: 0.5414 - val_loss: 0.6947 - val_acc: 0.5126\n",
      "Epoch 3/20\n",
      "3576/3576 [==============================] - 0s 73us/step - loss: 0.6885 - acc: 0.5414 - val_loss: 0.6933 - val_acc: 0.5126\n",
      "Epoch 4/20\n",
      "3576/3576 [==============================] - 0s 73us/step - loss: 0.6860 - acc: 0.5414 - val_loss: 0.6905 - val_acc: 0.5126\n",
      "Epoch 5/20\n",
      "3576/3576 [==============================] - 0s 73us/step - loss: 0.6825 - acc: 0.5411 - val_loss: 0.6873 - val_acc: 0.5101\n",
      "Epoch 6/20\n",
      "3576/3576 [==============================] - 0s 73us/step - loss: 0.6764 - acc: 0.5459 - val_loss: 0.6848 - val_acc: 0.6005\n",
      "Epoch 7/20\n",
      "3576/3576 [==============================] - 0s 73us/step - loss: 0.6675 - acc: 0.5615 - val_loss: 0.6816 - val_acc: 0.6005\n",
      "Epoch 8/20\n",
      "3576/3576 [==============================] - 0s 73us/step - loss: 0.6699 - acc: 0.5509 - val_loss: 0.6997 - val_acc: 0.5075\n",
      "Epoch 9/20\n",
      "3576/3576 [==============================] - 0s 73us/step - loss: 0.6651 - acc: 0.5579 - val_loss: 0.6845 - val_acc: 0.5955\n",
      "Epoch 10/20\n",
      "3576/3576 [==============================] - 0s 72us/step - loss: 0.6635 - acc: 0.5705 - val_loss: 0.6833 - val_acc: 0.5628\n",
      "Epoch 11/20\n",
      "3576/3576 [==============================] - 0s 73us/step - loss: 0.6632 - acc: 0.5621 - val_loss: 0.6854 - val_acc: 0.5854\n",
      "Epoch 12/20\n",
      "3576/3576 [==============================] - 0s 73us/step - loss: 0.6602 - acc: 0.5738 - val_loss: 0.6857 - val_acc: 0.5905\n",
      "Epoch 13/20\n",
      "3576/3576 [==============================] - 0s 73us/step - loss: 0.6600 - acc: 0.5794 - val_loss: 0.6857 - val_acc: 0.5879\n",
      "Epoch 14/20\n",
      "3576/3576 [==============================] - 0s 73us/step - loss: 0.6585 - acc: 0.5780 - val_loss: 0.6864 - val_acc: 0.5678\n",
      "Epoch 15/20\n",
      "3576/3576 [==============================] - 0s 72us/step - loss: 0.6590 - acc: 0.5800 - val_loss: 0.6862 - val_acc: 0.5980\n",
      "Epoch 16/20\n",
      "3576/3576 [==============================] - 0s 73us/step - loss: 0.6608 - acc: 0.5733 - val_loss: 0.6855 - val_acc: 0.5854\n",
      "Epoch 17/20\n",
      "3576/3576 [==============================] - 0s 73us/step - loss: 0.6591 - acc: 0.5786 - val_loss: 0.6944 - val_acc: 0.5477\n",
      "Epoch 18/20\n",
      "3576/3576 [==============================] - 0s 72us/step - loss: 0.6616 - acc: 0.5626 - val_loss: 0.6856 - val_acc: 0.5779\n",
      "Epoch 19/20\n",
      "3576/3576 [==============================] - 0s 73us/step - loss: 0.6606 - acc: 0.5789 - val_loss: 0.6873 - val_acc: 0.5754\n",
      "Epoch 20/20\n",
      "3576/3576 [==============================] - 0s 73us/step - loss: 0.6591 - acc: 0.5783 - val_loss: 0.6854 - val_acc: 0.5980\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fae7c26bb38>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now that we have the vectors, we can try to classify. \n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "\n",
    "# set parameters:\n",
    "batch_size = 128\n",
    "filters = 250\n",
    "kernel_size = 2\n",
    "hidden_dims = 250\n",
    "epochs = 20\n",
    "\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# we start off with an efficient embedding layer which maps\n",
    "# our vocab indices into embedding_dims dimensions\n",
    "#model.add(Embedding(max_features,\n",
    "#                    embedding_dims,\n",
    "#                    input_length=maxlen))\n",
    "\n",
    "\n",
    "#model.add(Dropout(0.2))\n",
    "\n",
    "# we add a Convolution1D, which will learn filters\n",
    "# word group filters of size filter_length:\n",
    "model.add(Conv1D(input_shape = (X.shape[1],X.shape[2]),\n",
    "                 filters=filters,\n",
    "                 kernel_size=kernel_size,\n",
    "                 padding='valid',\n",
    "                 activation='relu'))\n",
    "\n",
    "# Use max pooling:\n",
    "model.add(GlobalMaxPooling1D())\n",
    "\n",
    "# Add a vanilla hidden layer:\n",
    "model.add(Dense(hidden_dims))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# Add a vanilla hidden layer:\n",
    "model.add(Dense(50))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X, Y,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, ..., 0, 0, 1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.0351769 ],\n",
       "        [-0.09790957],\n",
       "        [-0.07749473],\n",
       "        ...,\n",
       "        [-0.05411887],\n",
       "        [-0.04910688],\n",
       "        [ 0.0098365 ]],\n",
       "\n",
       "       [[ 0.03633021],\n",
       "        [-0.09824814],\n",
       "        [-0.07865361],\n",
       "        ...,\n",
       "        [-0.05538593],\n",
       "        [-0.04867228],\n",
       "        [ 0.00954262]],\n",
       "\n",
       "       [[ 0.03600813],\n",
       "        [-0.09805113],\n",
       "        [-0.0790422 ],\n",
       "        ...,\n",
       "        [-0.0551894 ],\n",
       "        [-0.04906901],\n",
       "        [ 0.00975054]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 0.03733614],\n",
       "        [-0.09715127],\n",
       "        [-0.07879347],\n",
       "        ...,\n",
       "        [-0.05489784],\n",
       "        [-0.04980516],\n",
       "        [ 0.00744212]],\n",
       "\n",
       "       [[ 0.03541803],\n",
       "        [-0.09851338],\n",
       "        [-0.0780706 ],\n",
       "        ...,\n",
       "        [-0.05492842],\n",
       "        [-0.0492002 ],\n",
       "        [ 0.01005036]],\n",
       "\n",
       "       [[ 0.03725657],\n",
       "        [-0.09879603],\n",
       "        [-0.07782602],\n",
       "        ...,\n",
       "        [-0.05440046],\n",
       "        [-0.04904406],\n",
       "        [ 0.0088572 ]]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module fastText.FastText in fastText:\n",
      "\n",
      "NAME\n",
      "    fastText.FastText\n",
      "\n",
      "DESCRIPTION\n",
      "    # Copyright (c) 2017-present, Facebook, Inc.\n",
      "    # All rights reserved.\n",
      "    #\n",
      "    # This source code is licensed under the BSD-style license found in the\n",
      "    # LICENSE file in the root directory of this source tree. An additional grant\n",
      "    # of patent rights can be found in the PATENTS file in the same directory.\n",
      "\n",
      "FUNCTIONS\n",
      "    load_model(path)\n",
      "        Load a model given a filepath and return a model object.\n",
      "    \n",
      "    tokenize(text)\n",
      "        Given a string of text, tokenize it and return a list of tokens\n",
      "    \n",
      "    train_supervised(input, lr=0.1, dim=100, ws=5, epoch=5, minCount=1, minCountLabel=0, minn=0, maxn=0, neg=5, wordNgrams=1, loss='softmax', bucket=2000000, thread=12, lrUpdateRate=100, t=0.0001, label='__label__', verbose=2, pretrainedVectors='')\n",
      "        Train a supervised model and return a model object.\n",
      "        \n",
      "        input must be a filepath. The input text does not need to be tokenized\n",
      "        as per the tokenize function, but it must be preprocessed and encoded\n",
      "        as UTF-8. You might want to consult standard preprocessing scripts such\n",
      "        as tokenizer.perl mentioned here: http://www.statmt.org/wmt07/baseline.html\n",
      "        \n",
      "        The input file must must contain at least one label per line. For an\n",
      "        example consult the example datasets which are part of the fastText\n",
      "        repository such as the dataset pulled by classification-example.sh.\n",
      "    \n",
      "    train_unsupervised(input, model='skipgram', lr=0.05, dim=100, ws=5, epoch=5, minCount=5, minCountLabel=0, minn=3, maxn=6, neg=5, wordNgrams=1, loss='ns', bucket=2000000, thread=12, lrUpdateRate=100, t=0.0001, label='__label__', verbose=2, pretrainedVectors='')\n",
      "        Train an unsupervised model and return a model object.\n",
      "        \n",
      "        input must be a filepath. The input text does not need to be tokenized\n",
      "        as per the tokenize function, but it must be preprocessed and encoded\n",
      "        as UTF-8. You might want to consult standard preprocessing scripts such\n",
      "        as tokenizer.perl mentioned here: http://www.statmt.org/wmt07/baseline.html\n",
      "        \n",
      "        The input field must not contain any labels or use the specified label prefix\n",
      "        unless it is ok for those words to be ignored. For an example consult the\n",
      "        dataset pulled by the example script word-vector-example.sh, which is\n",
      "        part of the fastText repository.\n",
      "\n",
      "DATA\n",
      "    BOW = '<'\n",
      "    EOS = '</s>'\n",
      "    EOW = '>'\n",
      "    absolute_import = _Feature((2, 5, 0, 'alpha', 1), (3, 0, 0, 'alpha', 0...\n",
      "    division = _Feature((2, 2, 0, 'alpha', 2), (3, 0, 0, 'alpha', 0), 8192...\n",
      "    print_function = _Feature((2, 6, 0, 'alpha', 2), (3, 0, 0, 'alpha', 0)...\n",
      "    unicode_literals = _Feature((2, 6, 0, 'alpha', 2), (3, 0, 0, 'alpha', ...\n",
      "\n",
      "FILE\n",
      "    /opt/conda/lib/python3.5/site-packages/fastText/FastText.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(fastText.FastText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastText.util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package fastText.util in fastText:\n",
      "\n",
      "NAME\n",
      "    fastText.util\n",
      "\n",
      "DESCRIPTION\n",
      "    # Copyright (c) 2017-present, Facebook, Inc.\n",
      "    # All rights reserved.\n",
      "    #\n",
      "    # This source code is licensed under the BSD-style license found in the\n",
      "    # LICENSE file in the root directory of this source tree. An additional grant\n",
      "    # of patent rights can be found in the PATENTS file in the same directory.\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    util\n",
      "\n",
      "DATA\n",
      "    absolute_import = _Feature((2, 5, 0, 'alpha', 1), (3, 0, 0, 'alpha', 0...\n",
      "    division = _Feature((2, 2, 0, 'alpha', 2), (3, 0, 0, 'alpha', 0), 8192...\n",
      "    print_function = _Feature((2, 6, 0, 'alpha', 2), (3, 0, 0, 'alpha', 0)...\n",
      "    unicode_literals = _Feature((2, 6, 0, 'alpha', 2), (3, 0, 0, 'alpha', ...\n",
      "\n",
      "FILE\n",
      "    /opt/conda/lib/python3.5/site-packages/fastText/util/__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(fastText.util)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Object arrays are not currently supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-8394c7281988>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfastText\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_nearest_neighbor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CrookedHillary\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mft_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mban_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/fastText/util/util.py\u001b[0m in \u001b[0;36mfind_nearest_neighbor\u001b[0;34m(query, vectors, ban_set, cossims)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \"\"\"\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcossims\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mcossims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcossims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcossims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Object arrays are not currently supported"
     ]
    }
   ],
   "source": [
    "fastText.util.find_nearest_neighbor(\"CrookedHillary\",ft_model, ban_set='' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on _FastText in module fastText.FastText object:\n",
      "\n",
      "class _FastText(builtins.object)\n",
      " |  This class defines the API to inspect models and should not be used to\n",
      " |  create objects. It will be returned by functions such as load_model or\n",
      " |  train.\n",
      " |  \n",
      " |  In general this API assumes to be given only unicode for Python2 and the\n",
      " |  Python3 equvalent called str for any string-like arguments. All unicode\n",
      " |  strings are then encoded as UTF-8 and fed to the fastText C++ API.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, model=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  get_dimension(self)\n",
      " |      Get the dimension (size) of a lookup vector (hidden layer).\n",
      " |  \n",
      " |  get_input_matrix(self)\n",
      " |      Get a copy of the full input matrix of a Model. This only\n",
      " |      works if the model is not quantized.\n",
      " |  \n",
      " |  get_input_vector(self, ind)\n",
      " |      Given an index, get the corresponding vector of the Input Matrix.\n",
      " |  \n",
      " |  get_labels(self, include_freq=False)\n",
      " |      Get the entire list of labels of the dictionary optionally\n",
      " |      including the frequency of the individual labels. Unsupervised\n",
      " |      models use words as labels, which is why get_labels\n",
      " |      will call and return get_words for this type of\n",
      " |      model.\n",
      " |  \n",
      " |  get_line(self, text)\n",
      " |      Split a line of text into words and labels. Labels must start with\n",
      " |      the prefix used to create the model (__label__ by default).\n",
      " |  \n",
      " |  get_output_matrix(self)\n",
      " |      Get a copy of the full output matrix of a Model. This only\n",
      " |      works if the model is not quantized.\n",
      " |  \n",
      " |  get_sentence_vector(self, text)\n",
      " |      Given a string, get a single vector represenation. This function\n",
      " |      assumes to be given a single line of text. We split words on\n",
      " |      whitespace (space, newline, tab, vertical tab) and the control\n",
      " |      characters carriage return, formfeed and the null character.\n",
      " |  \n",
      " |  get_subword_id(self, subword)\n",
      " |      Given a subword, return the index (within input matrix) it hashes to.\n",
      " |  \n",
      " |  get_subwords(self, word)\n",
      " |      Given a word, get the subwords and their indicies.\n",
      " |  \n",
      " |  get_word_id(self, word)\n",
      " |      Given a word, get the word id within the dictionary.\n",
      " |      Returns -1 if word is not in the dictionary.\n",
      " |  \n",
      " |  get_word_vector(self, word)\n",
      " |      Get the vector representation of word.\n",
      " |  \n",
      " |  get_words(self, include_freq=False)\n",
      " |      Get the entire list of words of the dictionary optionally\n",
      " |      including the frequency of the individual words. This\n",
      " |      does not include any subwords. For that please consult\n",
      " |      the function get_subwords.\n",
      " |  \n",
      " |  is_quantized(self)\n",
      " |  \n",
      " |  predict(self, text, k=1, threshold=0.0)\n",
      " |      Given a string, get a list of labels and a list of\n",
      " |      corresponding probabilities. k controls the number\n",
      " |      of returned labels. A choice of 5, will return the 5\n",
      " |      most probable labels. By default this returns only\n",
      " |      the most likely label and probability. threshold filters\n",
      " |      the returned labels by a threshold on probability. A\n",
      " |      choice of 0.5 will return labels with at least 0.5\n",
      " |      probability. k and threshold will be applied together to\n",
      " |      determine the returned labels.\n",
      " |      \n",
      " |      This function assumes to be given\n",
      " |      a single line of text. We split words on whitespace (space,\n",
      " |      newline, tab, vertical tab) and the control characters carriage\n",
      " |      return, formfeed and the null character.\n",
      " |      \n",
      " |      If the model is not supervised, this function will throw a ValueError.\n",
      " |      \n",
      " |      If given a list of strings, it will return a list of results as usually\n",
      " |      received for a single line of text.\n",
      " |  \n",
      " |  quantize(self, input=None, qout=False, cutoff=0, retrain=False, epoch=None, lr=None, thread=None, verbose=None, dsub=2, qnorm=False)\n",
      " |      Quantize the model reducing the size of the model and\n",
      " |      it's memory footprint.\n",
      " |  \n",
      " |  save_model(self, path)\n",
      " |      Save the model to the given path\n",
      " |  \n",
      " |  test(self, path, k=1)\n",
      " |      Evaluate supervised model using file given by path\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(ft_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
