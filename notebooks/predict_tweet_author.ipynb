{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../src/data/raw_tweets.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  data['isTrump'] = [True if x == 'Twitter for Android' else False for x in data['source']]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "from data import raw_tweets\n",
    "\n",
    "all_data = raw_tweets.load()\n",
    "all_data = raw_tweets.label(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the stop words\n",
    "import pickle\n",
    "stop_words = pickle.load(open(\"../data/external/stop_words_en.pkl\", \"rb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "source\n",
       "Instagram                 2\n",
       "Media Studio              1\n",
       "Mobile Web (M5)           1\n",
       "Periscope                 1\n",
       "TweetDeck                 2\n",
       "Twitter Web Client      340\n",
       "Twitter for Android    1835\n",
       "Twitter for iPad         22\n",
       "Twitter for iPhone     1958\n",
       "Name: source, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.groupby('source')['source'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>id_str</th>\n",
       "      <th>in_reply_to_user_id_str</th>\n",
       "      <th>is_retweet</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>isTrump</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>2327</td>\n",
       "      <td>2327</td>\n",
       "      <td>2327</td>\n",
       "      <td>2</td>\n",
       "      <td>2327</td>\n",
       "      <td>2327</td>\n",
       "      <td>2327</td>\n",
       "      <td>2327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>1835</td>\n",
       "      <td>1835</td>\n",
       "      <td>1835</td>\n",
       "      <td>4</td>\n",
       "      <td>1835</td>\n",
       "      <td>1835</td>\n",
       "      <td>1835</td>\n",
       "      <td>1835</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         created_at  favorite_count  id_str  in_reply_to_user_id_str  \\\n",
       "isTrump                                                                \n",
       "False          2327            2327    2327                        2   \n",
       "True           1835            1835    1835                        4   \n",
       "\n",
       "         is_retweet  retweet_count  source  text  \n",
       "isTrump                                           \n",
       "False          2327           2327    2327  2327  \n",
       "True           1835           1835    1835  1835  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.groupby('isTrump').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-rUfVFkXDCja"
   },
   "source": [
    "## feature.py ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "ocVwE5l0DCjb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import gensim\n",
    "from gensim import corpora,models\n",
    "from gensim.models import Phrases\n",
    "from string import digits\n",
    "from nltk.corpus import words\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "import itertools\n",
    "import numpy as np\n",
    "from keras.utils import np_utils\n",
    "\n",
    "lemma = nltk.wordnet.WordNetLemmatizer()\n",
    "\n",
    "def clean_message(single_message):\n",
    "    single_message = single_message.lower()\n",
    "    sentence = [re.sub(\"[^a-zA-Z]\", \" \", word) for word in [single_message]]\n",
    "    sentence_word = [[lemma.lemmatize(i) for i in word_tokenize(word) if i not in string.punctuation and i not in digits and len(i) > 2 and i not in stop_words] for word in sentence]\n",
    "    sentence_word = list(itertools.chain(*sentence_word))\n",
    "    return sentence_word\n",
    "\n",
    "\n",
    "\n",
    "def build_feature (messages, vectorizer, train=True):\n",
    "    messages_clean=messages.map(clean_message)\n",
    "    \n",
    "    messages_clean_str=[]\n",
    "    for sen in messages_clean:\n",
    "        messages_clean_str.append(' '.join(str(e) for e in sen))\n",
    "    \n",
    "    if train:\n",
    "        vectorizer=vectorizer.fit(messages_clean_str)\n",
    "        with open('../models/vectorizer.pkl', 'wb') as f: \n",
    "            pickle.dump(vectorizer, f)\n",
    "        \n",
    "    features = vectorizer.transform(messages_clean_str)\n",
    "    features = features.todense()\n",
    "    \n",
    "    if train:\n",
    "        return np.array(features)\n",
    "    \n",
    "    else:\n",
    "        return features\n",
    "\n",
    "def save_encoder(label):\n",
    "    encoder = preprocessing.LabelEncoder()\n",
    "    encoder.fit(label)\n",
    "    np.save('classes.npy', encoder.classes_)\n",
    "    return encoder\n",
    "    \n",
    "    \n",
    "def one_hot_vec(var):\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.classes_ = np.load('classes.npy')\n",
    "    encoded_y = encoder.transform(var) #dummy code\n",
    "    dummy_y = np_utils.to_categorical(encoded_y)#one hot code\n",
    "    return dummy_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "82K2CPk4DCje"
   },
   "source": [
    "## train.py ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/keras/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/keras/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {}
     ]
    },
    "colab_type": "code",
    "id": "QS6Vlyd6DCje",
    "outputId": "f4d10c16-6ddd-4839-c211-1c1517179f14"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import string\n",
    "import nltk\n",
    "from gensim import corpora,models\n",
    "import gensim\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "#from nltk.corpus import stopwords\n",
    "#stopwords.words(\"english\")\n",
    "import re\n",
    "from nltk.stem import RegexpStemmer\n",
    "import pickle\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "#import tflearn\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_training_set(messages):\n",
    "    \n",
    "    # Creating Features from a Bag of Words\n",
    "    vectorizer = CountVectorizer(analyzer = \"word\",   \n",
    "                             tokenizer = None,    \n",
    "                             preprocessor = None, \n",
    "                             stop_words = None,   \n",
    "                             max_features = 2000,\n",
    "                             ngram_range=(1,2)) \n",
    "\n",
    "    training_set = build_feature(messages, vectorizer, train=True)\n",
    "    save_encoder(all_data['isTrump'])\n",
    "    return training_set, one_hot_vec(all_data['isTrump']), vectorizer\n",
    "    \n",
    "training_set, labels, vectorizer= create_training_set(all_data['text']) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4162\n",
      "2000\n",
      "4162\n",
      "2\n",
      "Train on 3329 samples, validate on 833 samples\n",
      "Epoch 1/5\n",
      " - 1s - loss: 0.4765 - acc: 0.7612 - val_loss: 0.5662 - val_acc: 0.7251\n",
      "Epoch 2/5\n",
      " - 0s - loss: 0.2762 - acc: 0.8822 - val_loss: 0.6048 - val_acc: 0.7455\n",
      "Epoch 3/5\n",
      " - 0s - loss: 0.1611 - acc: 0.9420 - val_loss: 0.7348 - val_acc: 0.7383\n",
      "Epoch 4/5\n",
      " - 0s - loss: 0.0780 - acc: 0.9724 - val_loss: 0.9578 - val_acc: 0.7323\n",
      "Epoch 5/5\n",
      " - 0s - loss: 0.0426 - acc: 0.9850 - val_loss: 1.0801 - val_acc: 0.7227\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from keras.models import Sequential\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "\n",
    "def save_model(model):\n",
    "    # saving model and weights\n",
    "    json_model = model.to_json()\n",
    "    open('model_architecture.json', 'w').write(json_model)\n",
    "    model.save_weights('model_weights.h5', overwrite=True)\n",
    "\n",
    "def load_model():\n",
    "    # loading model\n",
    "    model = model_from_json(open('model_architecture.json').read())\n",
    "    model.load_weights('model_weights.h5')\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "\n",
    "X=training_set\n",
    "Y=labels \n",
    "\n",
    "print(len(X))\n",
    "print(len(X[0]))\n",
    "print(len(Y))\n",
    "print(len(Y[0]))\n",
    "\n",
    "def build_model(optimizer='adam', dropout_rate=0.4, init='normal', \n",
    "                num_features = 1000, num_classes = 19,\n",
    "               units1 = 400, units2 = 400):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units1, input_dim=num_features, kernel_initializer=init, activation='relu'))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(units2, kernel_initializer=init, activation='relu'))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(num_classes, kernel_initializer=init, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "model=build_model(num_features = len(X[0]), num_classes = len(Y[0]), dropout_rate=0.4)\n",
    "model.fit(X, Y, epochs=5, batch_size=64, verbose=2, validation_split=0.2)\n",
    "save_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jc2O6Z4vDCjk"
   },
   "source": [
    "## grid serach for hyperparameters##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {}
     ]
    },
    "colab_type": "code",
    "id": "hb1ocEEZDCjk",
    "outputId": "5837703c-53a8-4fb2-896a-5ce1026991ef"
   },
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=build_model, verbose=0, num_features=len(X[0]), num_classes=len(Y[0]))\n",
    "# grid search epochs, batch size and optimizer...\n",
    "optimizers = ['adam']\n",
    "init = [ 'normal']\n",
    "epochs = [50]\n",
    "batches = [64,128]\n",
    "dropout_rate = [0.4]\n",
    "units1 = [40, 30]\n",
    "units2 = [40, 30]\n",
    "seed=10\n",
    "param_grid = dict(optimizer=optimizers, epochs=epochs, batch_size=batches, \n",
    "                  dropout_rate=dropout_rate, init=init,\n",
    "                 units1 = units1, units2 = units2)\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=kfold)\n",
    "grid_result = grid.fit(X, Y)\n",
    "\n",
    "#grid_result.cv_results_\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {}
     ]
    },
    "colab_type": "code",
    "id": "PBs8gs_uDCjn",
    "outputId": "0820c801-9c97-4669-c246-3960dae97ce0"
   },
   "outputs": [],
   "source": [
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {}
     ]
    },
    "colab_type": "code",
    "id": "D5qrTlpzDCjq",
    "outputId": "119beb34-67b4-4c3c-9c74-bbedae6c18e6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Txyqt99LDCjt"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "predict_topic_intent_keras-schnee.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
