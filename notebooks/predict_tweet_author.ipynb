{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "stop_words = pickle.load(open(\"../data/external/stop_words_en.pkl\", \"rb\"))\n",
    "TRAIN_FILENAME = \"../data/external/condensed_2016.json.zip\"\n",
    "\n",
    "all_data = pd.read_json(TRAIN_FILENAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "source\n",
       "Instagram                 2\n",
       "Media Studio              1\n",
       "Mobile Web (M5)           1\n",
       "Periscope                 1\n",
       "TweetDeck                 2\n",
       "Twitter Ads              63\n",
       "Twitter Web Client      340\n",
       "Twitter for Android    1835\n",
       "Twitter for iPad         22\n",
       "Twitter for iPhone     1958\n",
       "Name: source, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.groupby('source')['source'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = all_data.loc[all_data['source'] != 'Twitter Ads']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>id_str</th>\n",
       "      <th>in_reply_to_user_id_str</th>\n",
       "      <th>is_retweet</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>isTrump</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>2327</td>\n",
       "      <td>2327</td>\n",
       "      <td>2327</td>\n",
       "      <td>2</td>\n",
       "      <td>2327</td>\n",
       "      <td>2327</td>\n",
       "      <td>2327</td>\n",
       "      <td>2327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>1835</td>\n",
       "      <td>1835</td>\n",
       "      <td>1835</td>\n",
       "      <td>4</td>\n",
       "      <td>1835</td>\n",
       "      <td>1835</td>\n",
       "      <td>1835</td>\n",
       "      <td>1835</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         created_at  favorite_count  id_str  in_reply_to_user_id_str  \\\n",
       "isTrump                                                                \n",
       "False          2327            2327    2327                        2   \n",
       "True           1835            1835    1835                        4   \n",
       "\n",
       "         is_retweet  retweet_count  source  text  \n",
       "isTrump                                           \n",
       "False          2327           2327    2327  2327  \n",
       "True           1835           1835    1835  1835  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assume that only Android is authored by Trump. All else is not Trump.\n",
    "def isTrump(row):\n",
    "    if row['source'] == 'Twitter for Android' :\n",
    "      return True\n",
    "    return False\n",
    "\n",
    "all_data['isTrump'] = all_data.apply(lambda row: isTrump (row),axis=1)\n",
    "\n",
    "all_data.groupby('isTrump').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-rUfVFkXDCja"
   },
   "source": [
    "## feature.py ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "ocVwE5l0DCjb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import gensim\n",
    "from gensim import corpora,models\n",
    "from gensim.models import Phrases\n",
    "from string import digits\n",
    "from nltk.corpus import words\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "import itertools\n",
    "import numpy as np\n",
    "from keras.utils import np_utils\n",
    "\n",
    "lemma = nltk.wordnet.WordNetLemmatizer()\n",
    "\n",
    "def clean_message(single_message):\n",
    "    single_message = single_message.lower()\n",
    "    sentence = [re.sub(\"[^a-zA-Z]\", \" \", word) for word in [single_message]]\n",
    "    sentence_word = [[lemma.lemmatize(i) for i in word_tokenize(word) if i not in string.punctuation and i not in digits and len(i) > 2 and i not in stop_words] for word in sentence]\n",
    "    sentence_word = list(itertools.chain(*sentence_word))\n",
    "    return sentence_word\n",
    "\n",
    "\n",
    "\n",
    "def build_feature (messages, vectorizer, train=True):\n",
    "    messages_clean=messages.map(clean_message)\n",
    "    \n",
    "    messages_clean_str=[]\n",
    "    for sen in messages_clean:\n",
    "        messages_clean_str.append(' '.join(str(e) for e in sen))\n",
    "    \n",
    "    if train:\n",
    "        vectorizer=vectorizer.fit(messages_clean_str)\n",
    "        with open('../models/vectorizer.pkl', 'wb') as f: \n",
    "            pickle.dump(vectorizer, f)\n",
    "        \n",
    "    features = vectorizer.transform(messages_clean_str)\n",
    "    features = features.todense()\n",
    "    \n",
    "    if train:\n",
    "        return np.array(features)\n",
    "    \n",
    "    else:\n",
    "        return features\n",
    "\n",
    "def save_encoder(label):\n",
    "    encoder = preprocessing.LabelEncoder()\n",
    "    encoder.fit(label)\n",
    "    np.save('classes.npy', encoder.classes_)\n",
    "    return encoder\n",
    "    \n",
    "    \n",
    "def one_hot_vec(var):\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.classes_ = np.load('classes.npy')\n",
    "    encoded_y = encoder.transform(var) #dummy code\n",
    "    dummy_y = np_utils.to_categorical(encoded_y)#one hot code\n",
    "    return dummy_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "82K2CPk4DCje"
   },
   "source": [
    "## train.py ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/keras/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to /home/keras/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {}
     ]
    },
    "colab_type": "code",
    "id": "QS6Vlyd6DCje",
    "outputId": "f4d10c16-6ddd-4839-c211-1c1517179f14"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import string\n",
    "import nltk\n",
    "from gensim import corpora,models\n",
    "import gensim\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "#from nltk.corpus import stopwords\n",
    "#stopwords.words(\"english\")\n",
    "import re\n",
    "from nltk.stem import RegexpStemmer\n",
    "import pickle\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "#import tflearn\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_training_set(messages):\n",
    "    \n",
    "    # Creating Features from a Bag of Words\n",
    "    vectorizer = CountVectorizer(analyzer = \"word\",   \n",
    "                             tokenizer = None,    \n",
    "                             preprocessor = None, \n",
    "                             stop_words = None,   \n",
    "                             max_features = 2000,\n",
    "                             ngram_range=(1,2)) \n",
    "\n",
    "    training_set = build_feature(messages, vectorizer, train=True)\n",
    "    save_encoder(all_data['isTrump'])\n",
    "    return training_set, one_hot_vec(all_data['isTrump']), vectorizer\n",
    "    \n",
    "training_set, labels, vectorizer= create_training_set(all_data['text']) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [realdonaldtrump, happy, birthday, donaldjtrum...\n",
       "1       [happy, birthday, donaldjtrumpjr, http, urxycd...\n",
       "2       [happy, new, year, including, many, enemy, fou...\n",
       "3       [russian, playing, cnn, nbcnews, fool, funny, ...\n",
       "4       [join, american, founded, hall, fame, legend, ...\n",
       "5        [great, move, delay, putin, always, knew, smart]\n",
       "6       [administration, follow, two, simple, rule, ht...\n",
       "7       [economist, say, trump, delivered, hope, http,...\n",
       "8       [anymore, beginning, end, horrible, iran, deal...\n",
       "9       [continue, let, israel, treated, total, disdai...\n",
       "10      [best, disregard, many, inflammatory, presiden...\n",
       "11      [consumer, confidence, index, december, surged...\n",
       "12      [president, obama, campaigned, hard, personall...\n",
       "13      [djt, foundation, unlike, foundation, never, p...\n",
       "14      [gave, million, dollar, djt, foundation, raise...\n",
       "15      [world, gloomy, hope, market, nearly, christma...\n",
       "16      [united, nation, great, potential, right, club...\n",
       "17      [president, obama, said, think, would, say, sa...\n",
       "18         [merry, christmas, happy, new, year, everyone]\n",
       "19                      [merrychristmas, http, ggdmjrgms]\n",
       "20                 [foxnews, objectified, tonight, enjoy]\n",
       "21                      [happy, hanukkah, http, uvzwtykv]\n",
       "22      [big, loss, yesterday, israel, united, nation,...\n",
       "23      [nbcnews, purposely, left, part, nuclear, qout...\n",
       "24      [vladimir, putin, said, today, hillary, dems, ...\n",
       "25      [slaughter, purely, religious, threat, turned,...\n",
       "26      [terrorist, killed, many, people, germany, sai...\n",
       "27                                [thing, different, jan]\n",
       "28      [presidency, ridiculous, shame, love, kid, rai...\n",
       "29      [wonderful, son, eric, longer, allowed, raise,...\n",
       "                              ...                        \n",
       "4195    [namusca, realdonaldtrump, erictrump, diamonda...\n",
       "4196    [thank, biloxi, mississippi, remember, night, ...\n",
       "4197    [granite, hope, brandonstinney, deny, fact, hi...\n",
       "4198    [erictrump, proud, campaign, trail, realdonald...\n",
       "4199    [arrived, mississippi, rally, word, crowd, ove...\n",
       "4200    [heading, biloxi, mississippi, massive, crowd,...\n",
       "4201    [remember, self, funding, campaign, one, eithe...\n",
       "4202    [look, money, special, interest, lobbyist, giv...\n",
       "4203    [hope, bill, clinton, start, talking, woman, i...\n",
       "4204    [hillary, clinton, strength, stamen, president...\n",
       "4205    [hillary, clinton, said, ban, muslim, israel, ...\n",
       "4206    [jebbush, low, energy, stiff, focus, special, ...\n",
       "4207    [jebbush, sad, case, total, embarrassment, fam...\n",
       "4208    [massive, crowd, expected, mississippi, tomorr...\n",
       "4209    [votetrump, amp, together, makeamericagreataga...\n",
       "4210    [thank, much, http, omryruvet, naming, man, ye...\n",
       "4211    [person, hillary, clinton, least, want, run, f...\n",
       "4212    [codyraymille, never, interested, politics, wa...\n",
       "4213    [going, mississippi, tomorrow, night, hear, cr...\n",
       "4214    [casuperrunner, georgehenryw, huckabee, good, ...\n",
       "4215    [memeoryhead, one, biggest, fan, trump, wait, ...\n",
       "4216    [jodil, standing, spreading, word, trump, pres...\n",
       "4217    [marie, realdonaldtrump, love, trump, family, ...\n",
       "4218    [well, year, officially, begun, many, stop, pl...\n",
       "4219    [sprinklermanus, cnn, realdonaldtrump, spendin...\n",
       "4220    [jallenaip, hillary, said, fog, war, explanati...\n",
       "4221    [happy, new, year, maralago, thank, great, fam...\n",
       "4222             [happynewyearamerica, http, eeqb, pdrue]\n",
       "4223    [happy, new, year, amp, thank, http, qbzy, htt...\n",
       "4224    [foxnews, live, member, family, ring, new, yea...\n",
       "Name: text, Length: 4162, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data['text'].map(clean_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4162\n",
      "2000\n",
      "4162\n",
      "2\n",
      "Train on 3329 samples, validate on 833 samples\n",
      "Epoch 1/5\n",
      " - 1s - loss: 0.4815 - acc: 0.7516 - val_loss: 0.5581 - val_acc: 0.7203\n",
      "Epoch 2/5\n",
      " - 0s - loss: 0.2737 - acc: 0.8862 - val_loss: 0.5844 - val_acc: 0.7467\n",
      "Epoch 3/5\n",
      " - 0s - loss: 0.1555 - acc: 0.9423 - val_loss: 0.7547 - val_acc: 0.7215\n",
      "Epoch 4/5\n",
      " - 0s - loss: 0.0901 - acc: 0.9694 - val_loss: 0.9299 - val_acc: 0.7275\n",
      "Epoch 5/5\n",
      " - 0s - loss: 0.0447 - acc: 0.9850 - val_loss: 1.0959 - val_acc: 0.7215\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from keras.models import Sequential\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "\n",
    "def save_model(model):\n",
    "    # saving model and weights\n",
    "    json_model = model.to_json()\n",
    "    open('model_architecture.json', 'w').write(json_model)\n",
    "    model.save_weights('model_weights.h5', overwrite=True)\n",
    "\n",
    "def load_model():\n",
    "    # loading model\n",
    "    model = model_from_json(open('model_architecture.json').read())\n",
    "    model.load_weights('model_weights.h5')\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "\n",
    "X=training_set\n",
    "Y=labels \n",
    "\n",
    "print(len(X))\n",
    "print(len(X[0]))\n",
    "print(len(Y))\n",
    "print(len(Y[0]))\n",
    "\n",
    "def build_model(optimizer='adam', dropout_rate=0.4, init='normal', \n",
    "                num_features = 1000, num_classes = 19,\n",
    "               units1 = 400, units2 = 400):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units1, input_dim=num_features, kernel_initializer=init, activation='relu'))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(units2, kernel_initializer=init, activation='relu'))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(num_classes, kernel_initializer=init, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "model=build_model(num_features = len(X[0]), num_classes = len(Y[0]), dropout_rate=0.4)\n",
    "model.fit(X, Y, epochs=5, batch_size=64, verbose=2, validation_split=0.2)\n",
    "save_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jc2O6Z4vDCjk"
   },
   "source": [
    "## grid serach for hyperparameters##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {}
     ]
    },
    "colab_type": "code",
    "id": "hb1ocEEZDCjk",
    "outputId": "5837703c-53a8-4fb2-896a-5ce1026991ef"
   },
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=build_model, verbose=0, num_features=len(X[0]), num_classes=len(Y[0]))\n",
    "# grid search epochs, batch size and optimizer...\n",
    "optimizers = ['adam']\n",
    "init = [ 'normal']\n",
    "epochs = [50]\n",
    "batches = [64,128]\n",
    "dropout_rate = [0.4]\n",
    "units1 = [40, 30]\n",
    "units2 = [40, 30]\n",
    "seed=10\n",
    "param_grid = dict(optimizer=optimizers, epochs=epochs, batch_size=batches, \n",
    "                  dropout_rate=dropout_rate, init=init,\n",
    "                 units1 = units1, units2 = units2)\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=kfold)\n",
    "grid_result = grid.fit(X, Y)\n",
    "\n",
    "#grid_result.cv_results_\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {}
     ]
    },
    "colab_type": "code",
    "id": "PBs8gs_uDCjn",
    "outputId": "0820c801-9c97-4669-c246-3960dae97ce0"
   },
   "outputs": [],
   "source": [
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {}
     ]
    },
    "colab_type": "code",
    "id": "D5qrTlpzDCjq",
    "outputId": "119beb34-67b4-4c3c-9c74-bbedae6c18e6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Txyqt99LDCjt"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "predict_topic_intent_keras-schnee.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
